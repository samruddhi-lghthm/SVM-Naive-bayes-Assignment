{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Practical"
      ],
      "metadata": {
        "id": "ZWMiu6CuVy_R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Write a Python program to train an SVM Classifier on the Iris dataset and evaluate accuracy."
      ],
      "metadata": {
        "id": "64LKGaeCV8MG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the SVM classifier with a radial basis function (RBF) kernel\n",
        "svm_model = SVC(kernel='rbf', gamma='scale', C=1.0)\n",
        "\n",
        "# Train the SVM model\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Display a detailed classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "9iQuo8WhXtK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then\n",
        "compare their accuracies."
      ],
      "metadata": {
        "id": "5l90etv1ak_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the SVM classifiers with Linear and RBF kernels\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_rbf = SVC(kernel='rbf', gamma='scale', random_state=42)\n",
        "\n",
        "# Train the Linear SVM model\n",
        "svm_linear.fit(X_train, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "\n",
        "# Train the RBF SVM model\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# Display the accuracies\n",
        "print(f\"Accuracy of Linear SVM: {accuracy_linear:.2f}\")\n",
        "print(f\"Accuracy of RBF SVM: {accuracy_rbf:.2f}\")"
      ],
      "metadata": {
        "id": "8_1_KD9RafK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Write a Python program to train an SVM Regressor (SVR) on a housing dataset and evaluate it using Mean\n",
        "Squared Error (MSE)."
      ],
      "metadata": {
        "id": "NBJuUCH8btKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error, explained_variance_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the California housing dataset instead of the Boston housing dataset\n",
        "# as load_boston has been removed due to ethical concerns\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the SVR model with RBF kernel\n",
        "svr_model = SVR(kernel='rbf', C=1.0, epsilon=0.1, gamma='scale')\n",
        "\n",
        "# Train the SVR model\n",
        "svr_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = svr_model.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "\n",
        "# Calculate Explained Variance Score\n",
        "evs = explained_variance_score(y_test, y_pred)\n",
        "print(f\"Explained Variance Score: {evs:.2f}\")"
      ],
      "metadata": {
        "id": "AORVZUfdcuPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Write a Python program to train an SVM Classifier with a Polynomial Kernel and visualize the decision\n",
        "boundary."
      ],
      "metadata": {
        "id": "yCFgAkpedDX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Generate a synthetic 2D dataset\n",
        "# Changed n_informative to 2 to satisfy the constraint:\n",
        "# n_classes * n_clusters_per_class <= 2**n_informative\n",
        "# 2 * 2 <= 2**2 which is True\n",
        "X, y = datasets.make_classification(n_samples=100, n_features=2, n_classes=2, n_informative=2, n_redundant=0, n_repeated=0, random_state=42)\n",
        "\n",
        "# Standardize the dataset\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the SVM classifier with a Polynomial kernel\n",
        "svm_poly = SVC(kernel='poly', degree=3, C=1.0, coef0=1, random_state=42)\n",
        "\n",
        "# Train the SVM model\n",
        "svm_poly.fit(X_train, y_train)\n",
        "\n",
        "# Create a mesh grid for plotting\n",
        "xx, yy = np.meshgrid(np.linspace(X[:, 0].min() - 1, X[:, 0].max() + 1, 100),\n",
        "                     np.linspace(X[:, 1].min() - 1, X[:, 1].max() + 1, 100))\n",
        "\n",
        "# Predict over the mesh grid\n",
        "Z = svm_poly.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# Plot the decision boundary\n",
        "plt.contourf(xx, yy, Z, alpha=0.75, cmap=plt.cm.coolwarm)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', marker='o', s=50, cmap=plt.cm.coolwarm)\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('SVM Classifier with Polynomial Kernel')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xJ8oM6GHd0Dt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and\n",
        "evaluate accuracy."
      ],
      "metadata": {
        "id": "EE53TVxCen9z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Gaussian Naïve Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "\n",
        "# Train the classifier\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Evaluate the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Display the classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "uwEkbia5el3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Write a Python program to train a Multinomial Naïve Bayes classifier for text classification using the 20\n",
        "Newsgroups dataset."
      ],
      "metadata": {
        "id": "GQBJw65YfpCc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the 20 Newsgroups dataset\n",
        "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(newsgroups.data, newsgroups.target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the CountVectorizer to convert text to a bag-of-words representation\n",
        "vectorizer = CountVectorizer(stop_words='english', max_features=5000)\n",
        "\n",
        "# Fit and transform the training data, and transform the test data\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# Initialize the Multinomial Naïve Bayes classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "\n",
        "# Train the classifier\n",
        "nb_classifier.fit(X_train_vec, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = nb_classifier.predict(X_test_vec)\n",
        "\n",
        "# Evaluate the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Display the classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=newsgroups.target_names))"
      ],
      "metadata": {
        "id": "ColiCplbgugd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write a Python program to train an SVM Classifier with different C values and compare the decision boundaries visually."
      ],
      "metadata": {
        "id": "fxzXXaXwg5We"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 1: Generate a synthetic 2D dataset (binary classification)\n",
        "X, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, random_state=42)\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Define different values of C\n",
        "C_values = [0.1, 1, 10]\n",
        "\n",
        "# Step 4: Create a plot for each value of C and visualize the decision boundaries\n",
        "fig, axes = plt.subplots(1, len(C_values), figsize=(15, 5))\n",
        "\n",
        "for i, C in enumerate(C_values):\n",
        "    # Step 5: Train the SVM Classifier with the current value of C\n",
        "    svm_classifier = SVC(kernel='linear', C=C)\n",
        "    svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "    # Step 6: Plot the decision boundary for the current classifier\n",
        "    ax = axes[i]\n",
        "\n",
        "    # Plotting the decision boundary\n",
        "    h = .02  # Step size in the mesh\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "    Z = svm_classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    ax.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.coolwarm)\n",
        "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, edgecolors='k', marker='o', cmap=plt.cm.coolwarm)\n",
        "    ax.set_title(f\"SVM Classifier with C={C}\")\n",
        "    ax.set_xlabel('Feature 1')\n",
        "    ax.set_ylabel('Feature 2')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "4bLPgCOYiiQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Write a Python program to train a Bernoulli Naïve Bayes classifier for binary classification on a dataset with\n",
        "binary features."
      ],
      "metadata": {
        "id": "0YNJA-kOiisV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate a synthetic binary classification dataset with binary features\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=0,\n",
        "                           n_clusters_per_class=1, n_classes=2, random_state=42)\n",
        "\n",
        "# Convert the features to binary (0 or 1) by thresholding at 0.5\n",
        "X = (X > 0.5).astype(int)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Bernoulli Naive Bayes classifier\n",
        "bnb = BernoulliNB()\n",
        "\n",
        "# Train the model\n",
        "bnb.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = bnb.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "rO2g_I9fYzem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Write a Python program to apply feature scaling before training an SVM model and compare results with unscaled data."
      ],
      "metadata": {
        "id": "vjuAur5hY-pi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# --- SVM model on unscaled data ---\n",
        "svm_unscaled = SVC(kernel='linear', random_state=42)\n",
        "svm_unscaled.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate on unscaled data\n",
        "y_pred_unscaled = svm_unscaled.predict(X_test)\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "\n",
        "print(\"Results on unscaled data:\")\n",
        "print(f\"Accuracy: {accuracy_unscaled:.4f}\")\n",
        "print(classification_report(y_test, y_pred_unscaled))\n",
        "\n",
        "# --- Feature Scaling ---\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform the training data\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# Transform the test data using the same scaler\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# --- SVM model on scaled data ---\n",
        "svm_scaled = SVC(kernel='linear', random_state=42)\n",
        "svm_scaled.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict and evaluate on scaled data\n",
        "y_pred_scaled = svm_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "print(\"\\nResults on scaled data:\")\n",
        "print(f\"Accuracy: {accuracy_scaled:.4f}\")\n",
        "print(classification_report(y_test, y_pred_scaled))\n",
        "\n",
        "# --- Comparison ---\n",
        "print(\"\\nComparison of accuracies:\")\n",
        "print(f\"Accuracy on unscaled data: {accuracy_unscaled:.4f}\")\n",
        "print(f\"Accuracy on scaled data: {accuracy_scaled:.4f}\")"
      ],
      "metadata": {
        "id": "eMYZ-ZYDZN9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Write a Python program to train a Gaussian Naïve Bayes model and compare the predictions before and after Laplace Smoothing."
      ],
      "metadata": {
        "id": "yq8jpR61kxdi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the Iris dataset (you can replace this with any dataset)\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# --- Gaussian Naive Bayes model without smoothing ---\n",
        "gnb_no_smoothing = GaussianNB(var_smoothing=1e-9)  # var_smoothing controls the smoothing (default is 1e-9, so no smoothing)\n",
        "gnb_no_smoothing.fit(X_train, y_train)\n",
        "\n",
        "# Predictions with the model without smoothing\n",
        "y_pred_no_smoothing = gnb_no_smoothing.predict(X_test)\n",
        "\n",
        "# --- Applying Laplace-like smoothing ---\n",
        "# For Gaussian Naïve Bayes, Laplace smoothing isn't directly applied, but we can control the variance smoothing (var_smoothing).\n",
        "# Increasing var_smoothing effectively adds a kind of smoothing by controlling the variance.\n",
        "gnb_with_smoothing = GaussianNB(var_smoothing=1.0)  # Set a higher value for smoothing\n",
        "gnb_with_smoothing.fit(X_train, y_train)\n",
        "\n",
        "# Predictions with the model after smoothing\n",
        "y_pred_with_smoothing = gnb_with_smoothing.predict(X_test)\n",
        "\n",
        "# --- Comparison ---\n",
        "accuracy_no_smoothing = accuracy_score(y_test, y_pred_no_smoothing)\n",
        "accuracy_with_smoothing = accuracy_score(y_test, y_pred_with_smoothing)\n",
        "\n",
        "print(f\"Accuracy without smoothing: {accuracy_no_smoothing:.4f}\")\n",
        "print(f\"Accuracy with smoothing: {accuracy_with_smoothing:.4f}\")\n",
        "\n",
        "# Print the classification report for both models\n",
        "print(\"\\nClassification Report without Smoothing:\")\n",
        "print(classification_report(y_test, y_pred_no_smoothing))\n",
        "\n",
        "print(\"\\nClassification Report with Smoothing:\")\n",
        "print(classification_report(y_test, y_pred_with_smoothing))\n"
      ],
      "metadata": {
        "id": "dn9MUyRQao0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 11. Write a Python program to train an SVM Classifier and use GridSearchCV to tune the hyperparameters (C,gamma, kernel)."
      ],
      "metadata": {
        "id": "ypRoScDGlFA7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Step 1: Load the Iris dataset (you can replace this with your dataset)\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets (70% training, 30% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Set up the SVM classifier\n",
        "svm = SVC()\n",
        "\n",
        "# Step 4: Define the hyperparameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],        # Regularization parameter\n",
        "    'gamma': [0.001, 0.01, 0.1, 1],  # Kernel coefficient\n",
        "    'kernel': ['linear', 'rbf', 'poly']  # Types of kernels\n",
        "}\n",
        "\n",
        "# Step 5: Use GridSearchCV to find the best parameters (using 5-fold cross-validation)\n",
        "grid_search = GridSearchCV(estimator=svm, param_grid=param_grid, cv=5, verbose=1, n_jobs=-1)\n",
        "\n",
        "# Fit the model to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Get the best parameters and the best model\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Step 7: Make predictions using the best model\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Step 8: Evaluate the performance of the best model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Best Parameters: {best_params}\")\n",
        "print(f\"Accuracy of the best model: {accuracy:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "4mNJBciLbE9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. Write a Python program to train an SVM Classifier on an imbalanced dataset and apply class weighting andcheck it improve accuracy."
      ],
      "metadata": {
        "id": "JG0Znf_DllWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Create an imbalanced dataset by removing samples from the majority class\n",
        "# We will remove some samples of the majority class (class 1: Versicolor)\n",
        "X_imbalanced = X[y != 1]  # Remove class 1 samples\n",
        "y_imbalanced = y[y != 1]\n",
        "\n",
        "# Step 3: Split the imbalanced dataset into training and testing sets (70% training, 30% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_imbalanced, y_imbalanced, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 4: Train the SVM Classifier without class weights\n",
        "svm_classifier = SVC(kernel='linear', random_state=42)\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make predictions on the test set\n",
        "y_pred = svm_classifier.predict(X_test)\n",
        "\n",
        "# Step 6: Calculate the accuracy without class weights\n",
        "accuracy_no_weight = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy without class weights: {accuracy_no_weight:.4f}\")\n",
        "\n",
        "# Step 7: Compute class weights based on the training data\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weight_dict = dict(zip(np.unique(y_train), class_weights))\n",
        "\n",
        "# Step 8: Train the SVM Classifier with class weights\n",
        "svm_classifier_weighted = SVC(kernel='linear', class_weight=class_weight_dict, random_state=42)\n",
        "svm_classifier_weighted.fit(X_train, y_train)\n",
        "\n",
        "# Step 9: Make predictions on the test set\n",
        "y_pred_weighted = svm_classifier_weighted.predict(X_test)\n",
        "\n",
        "# Step 10: Calculate the accuracy with class weights\n",
        "accuracy_weighted = accuracy_score(y_test, y_pred_weighted)\n",
        "print(f\"Accuracy with class weights: {accuracy_weighted:.4f}\")\n"
      ],
      "metadata": {
        "id": "kg451yDVl2II"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 13. Write a Python program to implement a Naïve Bayes classifier for spam detection using email data."
      ],
      "metadata": {
        "id": "r0JFTxcOfUd6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Step 1: Load the SMS Spam Collection dataset\n",
        "# You can download the dataset from https://archive.ics.uci.edu/ml/datasets/sms+spam+collection\n",
        "# For this example, let's assume it's a CSV file with 'label' and 'message' columns.\n",
        "df = pd.read_csv('SMSSpamCollection', sep='\\t', header=None, names=['label', 'message'])\n",
        "\n",
        "# Step 2: Preprocess the data\n",
        "# We will use a simple split: ham = not spam and spam = 1\n",
        "df['label'] = df['label'].map({'ham': 0, 'spam': 1})\n",
        "\n",
        "# Step 3: Split the dataset into features (X) and labels (y)\n",
        "X = df['message']\n",
        "y = df['label']\n",
        "\n",
        "# Step 4: Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 5: Vectorize the text data using TF-IDF\n",
        "tfidf = TfidfVectorizer(stop_words='english')\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf.transform(X_test)\n",
        "\n",
        "# Step 6: Train the Naïve Bayes classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Step 7: Make predictions on the test set\n",
        "y_pred = nb_classifier.predict(X_test_tfidf)\n",
        "\n",
        "# Step 8: Evaluate the performance of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Classification Report (Precision, Recall, F1-score)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "hl-3K1wCmdLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. Write a Python program to train an SVM Classifier and a Naïve Bayes Classifier on the same dataset and compare their accuracy."
      ],
      "metadata": {
        "id": "bP84Nzzse9HH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets (70% training, 30% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train the SVM Classifier\n",
        "svm_classifier = SVC(kernel='rbf', random_state=42)\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Train the Naïve Bayes Classifier\n",
        "nb_classifier = GaussianNB()\n",
        "nb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make predictions with both classifiers\n",
        "y_pred_svm = svm_classifier.predict(X_test)\n",
        "y_pred_nb = nb_classifier.predict(X_test)\n",
        "\n",
        "# Step 6: Evaluate and compare the accuracy\n",
        "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
        "accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
        "\n",
        "print(f\"Accuracy of SVM classifier: {accuracy_svm:.4f}\")\n",
        "print(f\"Accuracy of Naïve Bayes classifier: {accuracy_nb:.4f}\")"
      ],
      "metadata": {
        "id": "ZuKem8eZektG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. Write a Python program to perform feature selection before training a Naïve Bayes classifier and compare results."
      ],
      "metadata": {
        "id": "ztsjq9c2f8RD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets (70% training, 30% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train the Naïve Bayes classifier without feature selection\n",
        "nb_classifier = GaussianNB()\n",
        "nb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make predictions and evaluate the performance without feature selection\n",
        "y_pred_no_fs = nb_classifier.predict(X_test)\n",
        "accuracy_no_fs = accuracy_score(y_test, y_pred_no_fs)\n",
        "\n",
        "print(f\"Accuracy without feature selection: {accuracy_no_fs:.4f}\")\n",
        "\n",
        "# Step 5: Perform Feature Selection using SelectKBest and chi-squared test\n",
        "# Select the top 2 features based on the chi-squared test\n",
        "select_k_best = SelectKBest(chi2, k=2)\n",
        "X_train_selected = select_k_best.fit_transform(X_train, y_train)\n",
        "X_test_selected = select_k_best.transform(X_test)\n",
        "\n",
        "# Step 6: Train the Naïve Bayes classifier with the selected features\n",
        "nb_classifier.fit(X_train_selected, y_train)\n",
        "\n",
        "# Step 7: Make predictions and evaluate the performance with feature selection\n",
        "y_pred_fs = nb_classifier.predict(X_test_selected)\n",
        "accuracy_fs = accuracy_score(y_test, y_pred_fs)\n",
        "\n",
        "print(f\"Accuracy with feature selection: {accuracy_fs:.4f}\")\n"
      ],
      "metadata": {
        "id": "NeCHPYTkglSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. Write a Python program to train an SVM Classifier using One-vs-Rest (OvR) and One-vs-One (OvO) strategies on the Wine dataset and compare their accuracy"
      ],
      "metadata": {
        "id": "M6BcvTR9gpPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets (70% training, 30% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train the SVM Classifier using the One-vs-Rest strategy (OvR)\n",
        "svm_ovr = SVC(decision_function_shape='ovr', random_state=42)  # One-vs-Rest\n",
        "svm_ovr.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Train the SVM Classifier using the One-vs-One strategy (OvO)\n",
        "svm_ovo = SVC(decision_function_shape='ovo', random_state=42)  # One-vs-One\n",
        "svm_ovo.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make predictions and evaluate the performance using both strategies\n",
        "y_pred_ovr = svm_ovr.predict(X_test)\n",
        "y_pred_ovo = svm_ovo.predict(X_test)\n",
        "\n",
        "accuracy_ovr = accuracy_score(y_test, y_pred_ovr)\n",
        "accuracy_ovo = accuracy_score(y_test, y_pred_ovo)\n",
        "\n",
        "# Step 6: Print the accuracy results\n",
        "print(f\"Accuracy with One-vs-Rest (OvR) strategy: {accuracy_ovr:.4f}\")\n",
        "print(f\"Accuracy with One-vs-One (OvO) strategy: {accuracy_ovo:.4f}\")"
      ],
      "metadata": {
        "id": "8cMFMbbxgmdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. Write a Python program to train an SVM Classifier using Linear, Polynomial, and RBF kernels on the Breast Cancer dataset and compare their accuracy."
      ],
      "metadata": {
        "id": "2jIBy6JehIOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets (70% training, 30% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train the SVM Classifier using the Linear kernel\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_linear.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Train the SVM Classifier using the Polynomial kernel\n",
        "svm_poly = SVC(kernel='poly', degree=3, random_state=42)  # Degree=3 is a common choice for polynomial kernels\n",
        "svm_poly.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Train the SVM Classifier using the RBF kernel\n",
        "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Make predictions and evaluate the performance using all three kernels\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "y_pred_poly = svm_poly.predict(X_test)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "accuracy_poly = accuracy_score(y_test, y_pred_poly)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# Step 7: Print the accuracy results\n",
        "print(f\"Accuracy with Linear kernel: {accuracy_linear:.4f}\")\n",
        "print(f\"Accuracy with Polynomial kernel: {accuracy_poly:.4f}\")\n",
        "print(f\"Accuracy with RBF kernel: {accuracy_rbf:.4f}\")"
      ],
      "metadata": {
        "id": "FSkwKPvcg9II"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. Write a Python program to train an SVM Classifier using Stratified K-Fold Cross-Validation and compute the average accuracy."
      ],
      "metadata": {
        "id": "I8Q3IhF2hoUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Initialize the SVM Classifier (using a linear kernel for simplicity)\n",
        "svm_classifier = SVC(kernel='linear', random_state=42)\n",
        "\n",
        "# Step 3: Initialize Stratified K-Fold Cross-Validation with 5 splits\n",
        "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Step 4: Perform cross-validation and compute the accuracy for each fold\n",
        "accuracies = cross_val_score(svm_classifier, X, y, cv=stratified_kfold, scoring='accuracy')\n",
        "\n",
        "# Step 5: Compute the average accuracy\n",
        "average_accuracy = np.mean(accuracies)\n",
        "\n",
        "# Step 6: Print the results\n",
        "print(f\"Accuracy for each fold: {accuracies}\")\n",
        "print(f\"Average accuracy: {average_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "7l5AFnxVhjG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. Write a Python program to train a Naïve Bayes classifier using different prior probabilities and compare performance."
      ],
      "metadata": {
        "id": "cRBaKbNQiGem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets (70% training, 30% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Define different sets of prior probabilities\n",
        "# 1. Uniform prior probabilities (equal for each class)\n",
        "uniform_priors = [1/3, 1/3, 1/3]\n",
        "\n",
        "# 2. Prior probabilities based on class distribution in the dataset\n",
        "class_probs = np.bincount(y_train) / len(y_train)\n",
        "\n",
        "# 3. Custom prior probabilities (for example, giving higher weight to class 0)\n",
        "custom_priors = [0.5, 0.3, 0.2]\n",
        "\n",
        "# Step 4: Train Naïve Bayes classifier with different prior probabilities\n",
        "\n",
        "# 1. Naïve Bayes with uniform prior\n",
        "nb_uniform = GaussianNB(priors=uniform_priors)\n",
        "nb_uniform.fit(X_train, y_train)\n",
        "y_pred_uniform = nb_uniform.predict(X_test)\n",
        "accuracy_uniform = accuracy_score(y_test, y_pred_uniform)\n",
        "\n",
        "# 2. Naïve Bayes with class-based prior probabilities\n",
        "nb_class_probs = GaussianNB(priors=class_probs)\n",
        "nb_class_probs.fit(X_train, y_train)\n",
        "y_pred_class_probs = nb_class_probs.predict(X_test)\n",
        "accuracy_class_probs = accuracy_score(y_test, y_pred_class_probs)\n",
        "\n",
        "# 3. Naïve Bayes with custom prior probabilities\n",
        "nb_custom = GaussianNB(priors=custom_priors)\n",
        "nb_custom.fit(X_train, y_train)\n",
        "y_pred_custom = nb_custom.predict(X_test)\n",
        "accuracy_custom = accuracy_score(y_test, y_pred_custom)\n",
        "\n",
        "# Step 5: Print the results and compare the performance\n",
        "print(f\"Accuracy with uniform prior probabilities: {accuracy_uniform:.4f}\")\n",
        "print(f\"Accuracy with class-based prior probabilities: {accuracy_class_probs:.4f}\")\n",
        "print(f\"Accuracy with custom prior probabilities: {accuracy_custom:.4f}\")\n"
      ],
      "metadata": {
        "id": "0dg-EIZniDzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. Write a Python program to perform Recursive Feature Elimination (RFE) before training an SVM Classifier and compare accuracy."
      ],
      "metadata": {
        "id": "kphC5dpIi1OL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets (70% training, 30% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train an SVM Classifier on the full set of features\n",
        "svm_full = SVC(kernel='linear', random_state=42)\n",
        "svm_full.fit(X_train, y_train)\n",
        "y_pred_full = svm_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Step 4: Apply Recursive Feature Elimination (RFE) to select the best features\n",
        "# We will use an SVM classifier as the estimator for RFE\n",
        "selector = RFE(estimator=SVC(kernel='linear', random_state=42), n_features_to_select=2)\n",
        "selector = selector.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Train an SVM Classifier using the selected features from RFE\n",
        "X_train_rfe = selector.transform(X_train)\n",
        "X_test_rfe = selector.transform(X_test)\n",
        "\n",
        "svm_rfe = SVC(kernel='linear', random_state=42)\n",
        "svm_rfe.fit(X_train_rfe, y_train)\n",
        "y_pred_rfe = svm_rfe.predict(X_test_rfe)\n",
        "accuracy_rfe = accuracy_score(y_test, y_pred_rfe)\n",
        "\n",
        "# Step 6: Print the results and compare the accuracy\n",
        "print(f\"Accuracy with all features: {accuracy_full:.4f}\")\n",
        "print(f\"Accuracy with selected features (RFE): {accuracy_rfe:.4f}\")\n"
      ],
      "metadata": {
        "id": "yelwgKEdihF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "21.  Write a Python program to train an SVM Classifier and evaluate its performance using Precision, Recall, and F1-Score instead of accuracy."
      ],
      "metadata": {
        "id": "nKonhf6jjNK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets (70% training, 30% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train the SVM Classifier\n",
        "svm_classifier = SVC(kernel='linear', random_state=42)\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make predictions on the test set\n",
        "y_pred = svm_classifier.predict(X_test)\n",
        "\n",
        "# Step 5: Evaluate performance using Precision, Recall, and F1-Score\n",
        "\n",
        "# As it's a multi-class classification problem, we will compute metrics per class and then calculate the average.\n",
        "precision = precision_score(y_test, y_pred, average='weighted')  # weighted average for multi-class\n",
        "recall = recall_score(y_test, y_pred, average='weighted')  # weighted average for multi-class\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')  # weighted average for multi-class\n",
        "\n",
        "# Step 6: Print the results\n",
        "print(f\"Precision (Weighted Average): {precision:.4f}\")\n",
        "print(f\"Recall (Weighted Average): {recall:.4f}\")\n",
        "print(f\"F1-Score (Weighted Average): {f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "_sr_ymqNjL4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Write a Python program to train a Naïve Bayes Classifier and evaluate its performance using Log Loss (Cross-Entropy Loss)."
      ],
      "metadata": {
        "id": "kvWu45qdjqmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets (70% training, 30% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train the Naïve Bayes Classifier\n",
        "nb_classifier = GaussianNB()\n",
        "nb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict probabilities (needed for Log Loss)\n",
        "y_prob = nb_classifier.predict_proba(X_test)\n",
        "\n",
        "# Step 5: Compute the Log Loss (Cross-Entropy Loss)\n",
        "log_loss_value = log_loss(y_test, y_prob)\n",
        "\n",
        "# Step 6: Print the Log Loss\n",
        "print(f\"Log Loss: {log_loss_value:.4f}\")\n"
      ],
      "metadata": {
        "id": "BwbMQeEZkCCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. Write a Python program to train an SVM Classifier and visualize the Confusion Matrix using seaborn."
      ],
      "metadata": {
        "id": "v9QKB8l-kJgr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets (70% training, 30% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train the SVM Classifier\n",
        "svm_classifier = SVC(kernel='linear', random_state=42)\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make predictions on the test set\n",
        "y_pred = svm_classifier.predict(X_test)\n",
        "\n",
        "# Step 5: Compute the Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Step 6: Visualize the Confusion Matrix using Seaborn's heatmap\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=data.target_names, yticklabels=data.target_names)\n",
        "plt.title('Confusion Matrix for SVM Classifier')\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tlqjfhzXkHnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Write a Python program to train an SVM Regressor (SVR) and evaluate its performance using Mean Absolute Error (MAE) instead of MSE."
      ],
      "metadata": {
        "id": "VWCB5kdpm8Ym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "# Instead of load_boston, use fetch_california_housing\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Step 1: Load the California Housing dataset\n",
        "# load_boston is deprecated due to ethical concerns\n",
        "#housing = load_boston()\n",
        "housing = fetch_california_housing()  # Use this instead\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Feature scaling (Standardize the features) for SVR\n",
        "scaler_X = StandardScaler()\n",
        "X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "X_test_scaled = scaler_X.transform(X_test)\n",
        "\n",
        "# Step 4: Train an SVM Regressor (SVR) with RBF kernel\n",
        "svr = SVR(kernel='rbf', C=1e3, gamma=0.1)\n",
        "svr.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Step 5: Make predictions on the test set\n",
        "y_pred = svr.predict(X_test_scaled)\n",
        "\n",
        "# Step 6: Evaluate the performance using Mean Absolute Error (MAE)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")"
      ],
      "metadata": {
        "id": "ooaWqu-7nf7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Write a Python program to train a Naïve Bayes classifier and evaluate its performance using the ROC-AUC score."
      ],
      "metadata": {
        "id": "uncAgGmTnxzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Convert the multi-class problem into a binary classification problem\n",
        "# We will classify if the class is \"setosa\" (class 0) vs. \"not setosa\" (class 1)\n",
        "y_binary = (y == 0).astype(int)\n",
        "\n",
        "# Step 3: Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Train the Naïve Bayes classifier (GaussianNB for continuous data)\n",
        "nb_classifier = GaussianNB()\n",
        "nb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make predictions on the test set\n",
        "y_pred = nb_classifier.predict(X_test)\n",
        "y_prob = nb_classifier.predict_proba(X_test)[:, 1]  # Get the probability of the positive class (class 1)\n",
        "\n",
        "# Step 6: Evaluate performance using ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "\n",
        "# Step 7: Plot the ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "V-Tbf2eznuSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. Write a Python program to train an SVM Classifier and visualize the Precision-Recall Curve."
      ],
      "metadata": {
        "id": "5IblW7htokz7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "\n",
        "# Step 1: Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train an SVM classifier with an RBF kernel\n",
        "svm_classifier = SVC(kernel='rbf', probability=True, random_state=42)\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Get the probability estimates for the positive class\n",
        "y_prob = svm_classifier.predict_proba(X_test)[:, 1]  # Probability of the positive class\n",
        "\n",
        "# Step 5: Compute precision-recall curve\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_prob)\n",
        "\n",
        "# Step 6: Plot the Precision-Recall curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, color='b', label='Precision-Recall curve (AP = %0.2f)' % average_precision_score(y_test, y_prob))\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve for SVM Classifier')\n",
        "plt.legend(loc='best')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZKpViY8QoWEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WxZ1nCQTo5sB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}