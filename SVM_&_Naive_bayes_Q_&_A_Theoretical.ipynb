{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Theoretical"
      ],
      "metadata": {
        "id": "jqNmDxVtQQlY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  What is a Support Vector Machine (SVM)?"
      ],
      "metadata": {
        "id": "LxqTU21TIL6A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : A support vector machine (SVM) is a computer algorithm that uses a hyperplane to separate data into different classes. SVMs are a type of supervised machine learning model."
      ],
      "metadata": {
        "id": "JMNs1cW2I7VX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the difference between Hard Margin and Soft Margin SVM?"
      ],
      "metadata": {
        "id": "ZEb4p1SqJBEW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : The main difference between hard margin and soft margin support vector machines (SVMs) is how well they separate data points."
      ],
      "metadata": {
        "id": "wGa5iSDJJTh0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is the mathematical intuition behind SVM?"
      ],
      "metadata": {
        "id": "iWngBAitJwlg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : The mathematical intuition behind Support Vector Machines (SVM) is to find the optimal hyperplane that maximizes the margin between different classes in a dataset, meaning it aims to create the largest possible separation between data points belonging to different categories, by focusing on the \"support vectors\" which are the data points closest to the decision boundary, thus achieving robust classification with minimal misclassifications; this is done by solving an optimization problem where the goal is to maximize the distance between the hyperplane and the closest data points from each class."
      ],
      "metadata": {
        "id": "Ru8NU2kVKfle"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is the role of Lagrange Multipliers in SVM?"
      ],
      "metadata": {
        "id": "LHTljxuTKoYk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : In Support Vector Machines (SVM), Lagrange multipliers are used as a mathematical tool to optimize the margin between different classes by finding the optimal hyperplane, effectively acting as weights that balance the objective function of maximizing the margin while ensuring the constraints imposed by the support vectors are met; essentially, they help solve the constrained optimization problem that arises when finding the best separating hyperplane in SVM by assigning importance to each data point based on its proximity to the decision boundary.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "u3EufW6hLVji"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What are Support Vectors in SVM?"
      ],
      "metadata": {
        "id": "RA1WUzNWNHqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : In a support vector machine (SVM), support vectors are data points that are closest to the decision boundary. They are used to classify new data points into different classes."
      ],
      "metadata": {
        "id": "i_K99I4-NWh6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is a Support Vector Classifier (SVC)?"
      ],
      "metadata": {
        "id": "irHL4WhsNjBV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : A Support Vector Classifier (SVC) is a supervised machine learning algorithm, specifically a type of Support Vector Machine (SVM), used for classification tasks where it aims to find the optimal hyperplane that best separates data points belonging to different classes, maximizing the margin between them, with the key data points closest to the hyperplane called \"support vectors\" which play a crucial role in defining the decision boundary; essentially, it seeks to identify the best dividing line between different categories in a dataset."
      ],
      "metadata": {
        "id": "Bbb4m7umNsuS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What is a Support Vector Regressor (SVR)?"
      ],
      "metadata": {
        "id": "orZBUbP8OlvN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : A Support Vector Regressor (SVR) is a machine learning algorithm used for regression tasks, which is essentially an extension of Support Vector Machines (SVM) designed to predict continuous values rather than classifications; it aims to find a hyperplane in the feature space that maximizes the margin between predicted values and actual values, allowing for accurate predictions while minimizing the impact of noise in the data by focusing on the most critical data points called \"support vectors.\""
      ],
      "metadata": {
        "id": "TcymnSGFPMFK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is the Kernel Trick in SVM?"
      ],
      "metadata": {
        "id": "xXYoOVpjPO1t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : The kernel trick is a machine learning technique that allows Support Vector Machines (SVMs) to work in high-dimensional spaces without explicitly computing the coordinates. This technique is used to solve non-linear problems."
      ],
      "metadata": {
        "id": "4t0xMqSYPUnB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Compare Linear Kernel, Polynomial Kernel, and RBF Kernel?"
      ],
      "metadata": {
        "id": "Hr2M9U6VPeOn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : When comparing linear, polynomial, and RBF kernels, the key difference lies in the complexity of the decision boundary they create, with the linear kernel being the simplest, creating a straight line, while polynomial and RBF kernels can handle more complex non-linear relationships by mapping data to a higher dimensional space, making them suitable for intricate data patterns; however, this added complexity comes at the cost of higher computational expense compared to the linear kernel."
      ],
      "metadata": {
        "id": "mm8z9fBnPn8l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.  What is the effect of the C parameter in SVM?"
      ],
      "metadata": {
        "id": "g1GcDgvjP2GZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : In a Support Vector Machine (SVM), the \"C\" parameter controls the trade-off between maximizing the margin between classes and minimizing misclassifications on the training data; essentially, a higher C value prioritizes correct classification of training points, leading to a smaller margin, while a lower C value prioritizes a larger margin even if it means misclassifying a few points on the training set."
      ],
      "metadata": {
        "id": "KYoB-zoLP68m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What is the role of the Gamma parameter in RBF Kernel SVM?"
      ],
      "metadata": {
        "id": "JN-E-flnQdbA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : In an RBF kernel SVM, the Gamma parameter controls the \"reach\" or influence of a single data point, essentially defining how far a training example's influence extends when determining the decision boundary; a higher gamma value means a smaller influence radius, making the model more sensitive to individual data points, potentially leading to overfitting, while a lower gamma value results in a wider influence area, allowing for smoother decision boundaries."
      ],
      "metadata": {
        "id": "D9UrkiNY7zns"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n"
      ],
      "metadata": {
        "id": "fBb0pyLw8ObN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : A Naïve Bayes classifier is a probabilistic machine learning algorithm that uses Bayes' Theorem to classify data, and it's called \"Naïve\" because it makes a simplifying assumption that all features are completely independent of each other, which is rarely true in real-world scenarios, hence the term \"naïve\" implying a somewhat unrealistic assumption;."
      ],
      "metadata": {
        "id": "HAaJRMFJ8X4-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.  What is Bayes’ Theorem?"
      ],
      "metadata": {
        "id": "guERCilF8lxS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : Bayes' theorem is a mathematical formula that calculates how likely an event is, given that another event has already happened. It's also known as Bayes' rule or Bayes' law."
      ],
      "metadata": {
        "id": "vEkgAbr381P_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes."
      ],
      "metadata": {
        "id": "tyEzWOGJ9Q0L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : The key difference between Gaussian Naive Bayes, Multinomial Naive Bayes, and Bernoulli Naive Bayes lies in the type of data they are designed to handle: Gaussian Naive Bayes assumes continuous data following a normal distribution, Multinomial Naive Bayes is best for discrete counts (like word frequencies in text), and Bernoulli Naive Bayes is used for binary features where each attribute can only be present or not present (like whether a specific word appears in a document)."
      ],
      "metadata": {
        "id": "CzHMqfp69YRJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. When should you use Gaussian Naïve Bayes over other variants?"
      ],
      "metadata": {
        "id": "FcYVHTLl9xqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : Use Gaussian Naïve Bayes when your data features are continuous and follow a normal distribution (Gaussian distribution), meaning you are dealing with numerical data like measurements or sensor readings where the features are expected to be spread evenly around a central point; this variant is particularly effective for classification tasks involving continuous variables like predicting house prices based on features like area and number of bedrooms."
      ],
      "metadata": {
        "id": "DMM4pFxo-CNw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What are the key assumptions made by Naïve Bayes?"
      ],
      "metadata": {
        "id": "8DItbfAc-wg0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : It assumes that predictors in a Naïve Bayes model are conditionally independent, or unrelated to any of the other feature in the model. It also assumes that all features contribute equally to the outcome."
      ],
      "metadata": {
        "id": "lY_EhT_6-6l7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What are the advantages and disadvantages of Naïve Bayes?"
      ],
      "metadata": {
        "id": "fUCTN_a1_TM_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : The Naïve Bayes classifier has several advantages, including its simplicity, speed, and scalability. However, it also has some disadvantages, such as the \"zero-frequency problem\" and its assumption of independence between attributes."
      ],
      "metadata": {
        "id": "oHTU1KUH_e1d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18.  Why is Naïve Bayes a good choice for text classification?"
      ],
      "metadata": {
        "id": "EKVlYRYk_hwc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : Naïve Bayes is a good choice for text classification because it's a simple, efficient algorithm that performs well with large datasets and high-dimensional data like text documents, making it fast to train and classify text while still achieving decent accuracy, especially when using a \"bag-of-words\" representation where words are treated as independent features - which aligns with the Naïve Bayes assumption of feature independence;."
      ],
      "metadata": {
        "id": "fIbgcmxDACkf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. Compare SVM and Naïve Bayes for classification tasks."
      ],
      "metadata": {
        "id": "fdeNrQo5AyYY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : SVM is widely appreciated for its ability to handle complex and high-dimensional data, while Naive Bayes is known for its simplicity, speed, and impressive performance in various scenarios."
      ],
      "metadata": {
        "id": "ZotR9-0_A8Z9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. How does Laplace Smoothing help in Naïve Bayes?"
      ],
      "metadata": {
        "id": "0UEtYFbmBHAO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : Laplace Smoothing helps in Naïve Bayes by preventing the issue of zero probability calculations, which can occur when a feature is not observed in the training data for a specific class, ensuring that even unseen features contribute a small probability to the classification, thus improving the model's robustness, especially when dealing with limited data sets."
      ],
      "metadata": {
        "id": "oP5RE7uVBlEd"
      }
    }
  ]
}